{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80043cbb",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "Let's put what we've learned about Lucene this far into practice. We're going to code our very own Full Text Search engine by developing our own analyzer, inverted index, queries and relevance scorer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb7daa6",
   "metadata": {},
   "source": [
    "## Installing Pre-Requisites\n",
    "During the analyzing phase we will need to stem our tokens to ensure that different variations of a word `ie. brewery, breweries, brewing --> brew`. We are stripping the word of its suffix, storing only the root of the token in our inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c81d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install Pystemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca29ab7",
   "metadata": {},
   "source": [
    "## Building the Analyzer\n",
    "Every sequence of text that will be indexed will first need to be analyzed. If you recall from the github repository, an analyzer is just a combination of characer filter(s), tokenizer(s) and token filter(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0273a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Stemmer\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adff4f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tokenize function is responsible for taking our sequence of text and splitting them on white space to provide us with tokens.\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "# the lowercase filter is responsible for converting all of our tokens into lowercase\n",
    "def lowercase_filter(tokens):\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "# the punction filter is responsible for ridding our tokens of any punctuation\n",
    "def punctuation_filter(tokens):\n",
    "    PUNCTUATION = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    return [PUNCTUATION.sub('', token) for token in tokens]\n",
    "\n",
    "# the stem filter function is responsible for stemming our tokens (as described 2 cells above)\n",
    "def stem_filter(tokens):\n",
    "    STEMMER = Stemmer.Stemmer('english')\n",
    "    return STEMMER.stemWords(tokens)\n",
    "\n",
    "# the stopwords filter is meant to filter out common stopwords that can impact our search scoring and indexing\n",
    "def stopword_filter(tokens):\n",
    "    STOPWORDS = set(['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have',\n",
    "                     'I', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you',\n",
    "                     'do', 'at', 'this', 'but', 'his', 'by', 'from', 'wikipedia'])\n",
    "    return [token for token in tokens if token not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4394648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The analyze function is meant to put tokenizer and token filters together and execute. \n",
    "def analyze(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = lowercase_filter(tokens)\n",
    "    tokens = punctuation_filter(tokens)\n",
    "    tokens = stopword_filter(tokens)\n",
    "    tokens = stem_filter(tokens)\n",
    "\n",
    "    return [token for token in tokens if token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a134b1",
   "metadata": {},
   "source": [
    "## Testing the Analyzer we've built\n",
    "Let's run a sample sequence of text against the Analyzer we've built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91ad557",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(\"The quick brown fox jumps over the lazy dog.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b4003f",
   "metadata": {},
   "source": [
    "## Indexing a dataset\n",
    "The next step after we've built a working analyzer is to analyze some real data. Let's import a movies dataset from a JSON file and then write the index function needed to analyze and index the movie titles.\n",
    "\n",
    "In order to work with json files, we'll first need to import the json python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6e873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca198d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the movies collection as a dictionary\n",
    "filename = 'data/movies.json'\n",
    "with open(filename, 'r') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "# The index function will instantiate an empty dictionary before filling it with analyzed tokens from our dataset. \n",
    "# We will be assigning the token as the key of the dictionary and the object_ids as the value(s) of the key\n",
    "\n",
    "def index():\n",
    "    index={}\n",
    "    # for each movie, run the analyzer function above on title and add it to a set with the movies' ID\n",
    "    for document in documents:\n",
    "        for token in analyze(document['title']):\n",
    "            if token not in index:\n",
    "                index[token] = set()\n",
    "            index[token].add(document['_id']['$oid'])\n",
    "            \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb5f2a3",
   "metadata": {},
   "source": [
    "## Search\n",
    "Now we'll need to be able to define how we want to search against our dataset given our defined analyzer and inverted index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13ae08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Search function is responsible for taking in a query, analyzing it using the analyzer code from above, \n",
    "# and then retrieving the corresponding object_ids in our index that match with our tokens. From there we can \n",
    "# lookup all the movies in our dataset that we've matched against. \n",
    "def search(query):\n",
    "    # tokenize the query     \n",
    "    analyzed_query = analyze(query)\n",
    "    # grab movie tokens from the index that match the tokens from the query    \n",
    "    results = [index().get(token, set()) for token in analyzed_query]\n",
    "    \n",
    "    resulting_documents = []\n",
    "    \n",
    "    ids = set()\n",
    "    for result in results:\n",
    "        for singles in result:\n",
    "            ids.add(singles)\n",
    "    \n",
    "    # return all movies where the tokenized query matches the tokenized title\n",
    "    for single_id in ids:\n",
    "        for document in documents:\n",
    "            if document['_id']['$oid'] == single_id:\n",
    "                resulting_documents.append(document)\n",
    "    return resulting_documents\n",
    "    \n",
    "search(\"forrest gump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259da04d",
   "metadata": {},
   "source": [
    "## Scoring \n",
    "Recall that in order to score documents we need to calculate the term frequency and the inverse document frequency (TF-IDF). Typically we'll see these as separate functions within an index class, however for simplicity we're going to include those calculations within our search function itself. \n",
    "\n",
    "note the `tf`, `idf` and `score` variables now.\n",
    "\n",
    "note: be sure to check the comments in the code as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ace315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need math to do the idf calculation\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Search function is responsible for taking in a query, analyzing it using the analyzer code from above, \n",
    "# and then retrieving the corresponding object_ids in our index that match with our tokens. From there we can \n",
    "# lookup all the movies in our dataset that we've matched against. \n",
    "def search(query):\n",
    "    # tokenize the query     \n",
    "    analyzed_query = analyze(query)\n",
    "    # grab movie tokens from the index that match the tokens from the query    \n",
    "    results = [index().get(token, set()) for token in analyzed_query]\n",
    "    \n",
    "    resulting_documents = []\n",
    "    \n",
    "    ids = set()\n",
    "    for result in results:\n",
    "        for singles in result:\n",
    "            ids.add(singles)\n",
    "    \n",
    "    # return all movies where the tokenized query matches the tokenized title\n",
    "    for single_id in ids:\n",
    "        for document in documents:\n",
    "            if document['_id']['$oid'] == single_id:\n",
    "                score = 0.0\n",
    "                for token in analyzed_query:\n",
    "                    #normally you would want to analyze the title but for simplicy i'm just going to lower it. \n",
    "                    #since our analyzer includes a stemmer, the stemmed token should be included in the title if there's a match\n",
    "                    tf = document['title'].lower().count(token)\n",
    "                    idf = math.log10(len(documents) / len(index().get(token)))\n",
    "                    score += tf * idf\n",
    "                resulting_documents.append((document, score))\n",
    "            \n",
    "                # resulting_documents.append(document)\n",
    "    return sorted(resulting_documents, key=lambda doc: doc[1], reverse=True)\n",
    "    \n",
    "search(\"forrest gump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df576ea5",
   "metadata": {},
   "source": [
    "## Output\n",
    "The output will be an array of tuples where the first element of the tuple is the full document and the second element is the tf-idf score. \n",
    "\n",
    "As you can see, Forrest Gump returns the highest TF-IDF score\n",
    "Finding Forrester returns the second highest TF-IDF score\n",
    "\n",
    "\"Forrest Gump\" : 8.343408593803858\n",
    "\"Finding Forrester\" :  4.021189299069938"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "5bd3c9d9a8d06469e52398b8cc2bd7abd17576200cf663296ae7512321290c1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
